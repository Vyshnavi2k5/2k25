{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx976CNA7gOkp6TniSDpz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vyshnavi2k5/2k25/blob/main/copy_of_unsupervised_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unsupervised Learning Suite for Healthcare Dataset\n",
        "# Colab/Jupyter ready. Copy-paste entire cell and run.\n",
        "# Produces outputs in \"./unsupervised_reports/\" by default.\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "# Optional libraries (UMAP, mlxtend) - install if missing (Colab)\n",
        "try:\n",
        "    import umap\n",
        "except Exception:\n",
        "    try:\n",
        "        !pip install --quiet umap-learn\n",
        "        import umap\n",
        "    except Exception:\n",
        "        umap = None\n",
        "\n",
        "try:\n",
        "    from mlxtend.frequent_patterns import apriori, association_rules\n",
        "except Exception:\n",
        "    try:\n",
        "        !pip install --quiet mlxtend\n",
        "        from mlxtend.frequent_patterns import apriori, association_rules\n",
        "    except Exception:\n",
        "        apriori = None\n",
        "        association_rules = None\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "RANDOM_STATE = 42\n",
        "USE_SAMPLE = True          # Set False to run on full dataset (can be slow)\n",
        "SAMPLE_SIZE = 4000         # sample size if USE_SAMPLE=True\n",
        "TOP_K_CAT = 50             # keep top-K categories for each categorical column, rest -> 'Other'\n",
        "OUTPUT_DIR = \"unsupervised_reports\"\n",
        "SEARCH_PATHS = [\".\", \"/content\", \"/mnt/data\"]  # where to look for uploaded file\n",
        "# ----------------------------------------\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def find_input_file():\n",
        "    \"\"\"Search for a csv or zip with 'health' or fallback to any csv.\"\"\"\n",
        "    cand = []\n",
        "    for root in SEARCH_PATHS:\n",
        "        for p in Path(root).rglob(\"*\"):\n",
        "            if p.is_file():\n",
        "                name = p.name.lower()\n",
        "                if (\"health\" in name or \"healthcare\" in name) and (name.endswith(\".zip\") or name.endswith(\".csv\")):\n",
        "                    cand.append(str(p))\n",
        "    if not cand:\n",
        "        # fallback: any csv\n",
        "        for root in SEARCH_PATHS:\n",
        "            for p in Path(root).rglob(\"*.csv\"):\n",
        "                cand.append(str(p))\n",
        "    return cand[0] if cand else None\n",
        "\n",
        "def safe_ohe(**kwargs):\n",
        "    \"\"\"Return OneHotEncoder that works across sklearn versions.\"\"\"\n",
        "    try:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, **kwargs)\n",
        "    except TypeError:\n",
        "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False, **kwargs)\n",
        "\n",
        "def reduce_cardinality(df, col, top_k=TOP_K_CAT):\n",
        "    # Convert to string before value_counts to handle mixed types\n",
        "    df[col] = df[col].astype(str)\n",
        "    top = df[col].value_counts().nlargest(top_k).index\n",
        "    df[col] = df[col].where(df[col].isin(top), other=\"Other\")\n",
        "    return df\n",
        "\n",
        "def save_scatter(X2, labels, title, fname):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    labs = np.asarray(labels)\n",
        "    unique = np.unique(labs)\n",
        "    cmap = plt.cm.get_cmap(\"tab10\", max(3, len(unique)))\n",
        "    for i, u in enumerate(unique):\n",
        "        mask = (labs == u)\n",
        "        plt.scatter(X2[mask,0], X2[mask,1], s=12, alpha=0.7, label=str(u))\n",
        "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left', fontsize='small', markerscale=2)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, fname), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# ---------- Load dataset ----------\n",
        "input_file = find_input_file()\n",
        "if input_file is None:\n",
        "    raise FileNotFoundError(\"No input CSV/ZIP found. Upload 'Healthcare dataset.zip' or 'healthcare_dataset.csv' to Colab/Jupyter.\")\n",
        "print(\"Input file:\", input_file)\n",
        "\n",
        "if input_file.lower().endswith(\".zip\"):\n",
        "    with zipfile.ZipFile(input_file, \"r\") as z:\n",
        "        extract_dir = \"/tmp/healthcare_extract\"\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "        z.extractall(extract_dir)\n",
        "    csvs = list(Path(extract_dir).rglob(\"*.csv\"))\n",
        "    if not csvs:\n",
        "        raise FileNotFoundError(\"No CSV found inside the ZIP.\")\n",
        "    csv_path = str(csvs[0])\n",
        "else:\n",
        "    csv_path = input_file\n",
        "\n",
        "print(\"Loading CSV:\", csv_path)\n",
        "df = pd.read_csv(csv_path, low_memory=False)\n",
        "print(\"Rows loaded:\", len(df))\n",
        "\n",
        "# ---------- Optional sampling (speed) ----------\n",
        "if USE_SAMPLE and len(df) > SAMPLE_SIZE:\n",
        "    df = df.sample(SAMPLE_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "    print(\"Sampled rows:\", len(df))\n",
        "\n",
        "# ---------- Feature engineering (safe) ----------\n",
        "# parse dates if present\n",
        "date_cols_to_process = [\"Date of Admission\", \"Discharge Date\"]\n",
        "for c in date_cols_to_process:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "\n",
        "if (\"Date of Admission\" in df.columns) and (\"Discharge Date\" in df.columns):\n",
        "    df[\"LOS_Days\"] = (df[\"Discharge Date\"] - df[\"Date of Admission\"]).dt.days.clip(lower=0).fillna(0)\n",
        "\n",
        "if \"Date of Admission\" in df.columns:\n",
        "    df[\"Admission_Year\"] = pd.to_datetime(df[\"Date of Admission\"], errors=\"coerce\").dt.year.fillna(0).astype(int)\n",
        "    df[\"Admission_Month\"] = pd.to_datetime(df[\"Date of Admission\"], errors=\"coerce\").dt.month.fillna(0).astype(int)\n",
        "    df[\"Admission_DOW\"] = pd.to_datetime(df[\"Date of Admission\"], errors=\"coerce\").dt.dayofweek.fillna(0).astype(int)\n",
        "\n",
        "# drop obvious identifiers if present\n",
        "for col in [\"Name\", \"Patient ID\", \"Doctor\", \"Room Number\"]:\n",
        "    if col in df.columns:\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "\n",
        "# cast object cols to str & strip\n",
        "for c in df.select_dtypes(include=[\"object\"]).columns:\n",
        "    df[c] = df[c].astype(str).str.strip()\n",
        "\n",
        "# ---------- Detect numeric & categorical ----------\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove date columns from categorical_cols after processing them\n",
        "categorical_cols = [col for col in categorical_cols if col not in date_cols_to_process]\n",
        "numeric_cols = [col for col in numeric_cols if col not in date_cols_to_process]\n",
        "\n",
        "print(\"Numeric cols:\", numeric_cols)\n",
        "print(\"Categorical cols:\", categorical_cols)\n",
        "\n",
        "# ---------- Reduce cardinality for high-cardinality categoricals ----------\n",
        "for col in list(categorical_cols):\n",
        "    if df[col].nunique() > TOP_K_CAT:\n",
        "        df = reduce_cardinality(df, col, top_k=TOP_K_CAT)\n",
        "        print(f\"Reduced cardinality for {col}: now {df[col].nunique()} uniques\")\n",
        "\n",
        "# refresh lists after cardinality reduction (which modifies df in place)\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "# Remove date columns again in case they were re-added somehow (shouldn't happen, but for safety)\n",
        "categorical_cols = [col for col in categorical_cols if col not in date_cols_to_process]\n",
        "numeric_cols = [col for col in numeric_cols if col not in date_cols_to_process]\n",
        "\n",
        "\n",
        "if not numeric_cols and not categorical_cols:\n",
        "    raise ValueError(\"No usable numeric or categorical columns found in the dataset.\")\n",
        "\n",
        "# ---------- Preprocessing ----------\n",
        "ohe = safe_ohe()\n",
        "scaler = StandardScaler()\n",
        "transformers = []\n",
        "if numeric_cols:\n",
        "    transformers.append((\"num\", scaler, numeric_cols))\n",
        "if categorical_cols:\n",
        "    transformers.append((\"cat\", ohe, categorical_cols))\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "X = preprocessor.fit_transform(df)\n",
        "print(\"Feature matrix shape (after encoding):\", X.shape)\n",
        "\n",
        "# Save feature names (useful for explanations)\n",
        "feature_names = []\n",
        "if numeric_cols:\n",
        "    feature_names += numeric_cols\n",
        "if categorical_cols:\n",
        "    try:\n",
        "        cat_enc = preprocessor.named_transformers_[\"cat\"]\n",
        "        for i, col in enumerate(categorical_cols):\n",
        "            cats = cat_enc.categories_[i]\n",
        "            feature_names += [f\"{col}__{str(v)}\" for v in cats]\n",
        "    except Exception:\n",
        "        # fallback: generic names\n",
        "        feature_names += [f\"cat_{i}\" for i in range(X.shape[1] - len(feature_names))]\n",
        "\n",
        "# ---------- Dimensionality reduction ----------\n",
        "n_pca = min(30, X.shape[1])\n",
        "if n_pca <= 0:\n",
        "    raise ValueError(\"No features after preprocessing.\")\n",
        "pca = PCA(n_components=n_pca, random_state=RANDOM_STATE)\n",
        "X_pca = pca.fit_transform(X)\n",
        "print(\"PCA explained variance (cumulative first 5):\", np.cumsum(pca.explained_variance_ratio_)[:5])\n",
        "X_2d = X_pca[:, :2]\n",
        "pd.DataFrame(X_2d, columns=[\"pca1\", \"pca2\"]).to_csv(os.path.join(OUTPUT_DIR, \"embed_pca2.csv\"), index=False)\n",
        "\n",
        "# ---------- Clustering ----------\n",
        "cluster_summary = []\n",
        "\n",
        "def add_cluster_result(name, labels, X_for_metrics):\n",
        "    labels_arr = np.asarray(labels)\n",
        "    unique = np.unique(labels_arr)\n",
        "    n_clusters = len(unique)\n",
        "    sil = None\n",
        "    db = None\n",
        "    try:\n",
        "        # compute silhouette/db only on non-noise labels if present\n",
        "        mask = labels_arr != -1\n",
        "        if mask.sum() > 1 and len(np.unique(labels_arr[mask])) > 1:\n",
        "            sil = float(silhouette_score(X_for_metrics[mask], labels_arr[mask]))\n",
        "            db = float(davies_bouldin_score(X_for_metrics[mask], labels_arr[mask]))\n",
        "    except Exception:\n",
        "        sil = None\n",
        "        db = None\n",
        "    cluster_summary.append({\"method\": name, \"n_clusters\": int(n_clusters), \"silhouette\": sil, \"davies_bouldin\": db})\n",
        "\n",
        "# KMeans (several k)\n",
        "for k in [3, 4, 5, 6, 8]:\n",
        "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
        "    labs = km.fit_predict(X_pca)\n",
        "    add_cluster_result(f\"KMeans_k{k}\", labs, X_pca)\n",
        "    df[f\"kmeans_k{k}_cluster\"] = labs\n",
        "    save_scatter(X_2d, labs, f\"KMeans k={k}\", f\"kmeans_k{k}.png\")\n",
        "\n",
        "# Agglomerative\n",
        "for k in [3, 4, 5, 8]:\n",
        "    agg = AgglomerativeClustering(n_clusters=k)\n",
        "    labs = agg.fit_predict(X_pca)\n",
        "    add_cluster_result(f\"Agglomerative_k{k}\", labs, X_pca)\n",
        "    df[f\"agg_k{k}_cluster\"] = labs\n",
        "    save_scatter(X_2d, labs, f\"Agglomerative k={k}\", f\"agg_k{k}.png\")\n",
        "\n",
        "# DBSCAN - eps heuristic from k-distance\n",
        "nn = NearestNeighbors(n_neighbors=6).fit(X_pca)\n",
        "distances, _ = nn.kneighbors(X_pca)\n",
        "kdist = np.sort(distances[:, -1])\n",
        "eps_guess = float(np.percentile(kdist, 90))\n",
        "eps = eps_guess if eps_guess > 0 else 0.5\n",
        "db = DBSCAN(eps=eps, min_samples=5)\n",
        "labs = db.fit_predict(X_pca)\n",
        "add_cluster_result(f\"DBSCAN_eps{eps:.3f}\", labs, X_pca)\n",
        "df[\"dbscan_cluster\"] = labs\n",
        "save_scatter(X_2d, labs, f\"DBSCAN eps={eps:.3f}\", \"dbscan.png\")\n",
        "\n",
        "# Gaussian Mixture\n",
        "for k in [3,4,5,8]:\n",
        "    gm = GaussianMixture(n_components=k, random_state=RANDOM_STATE, n_init=3)\n",
        "    labs = gm.fit_predict(X_pca)\n",
        "    add_cluster_result(f\"GMM_k{k}\", labs, X_pca)\n",
        "    df[f\"gmm_k{k}_cluster\"] = labs\n",
        "    save_scatter(X_2d, labs, f\"GMM k={k}\", f\"gmm_k{k}.png\")\n",
        "\n",
        "# ---------- Outlier / Anomaly detection ----------\n",
        "# IsolationForest\n",
        "iso = IsolationForest(n_estimators=200, contamination=0.01, random_state=RANDOM_STATE)\n",
        "iso_preds = iso.fit_predict(X_pca)\n",
        "df[\"iso_anomaly\"] = iso_preds  # -1 anomaly, 1 normal\n",
        "pd.DataFrame({\"iso_pred\": iso_preds}).to_csv(os.path.join(OUTPUT_DIR, \"isolation_preds.csv\"), index=False)\n",
        "\n",
        "# LocalOutlierFactor\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01, novelty=False)\n",
        "lof_preds = lof.fit_predict(X_pca)\n",
        "df[\"lof_anomaly\"] = lof_preds\n",
        "pd.DataFrame({\"lof_pred\": lof_preds}).to_csv(os.path.join(OUTPUT_DIR, \"lof_preds.csv\"), index=False)\n",
        "\n",
        "# One-Class SVM (may be slow)\n",
        "try:\n",
        "    ocsvm = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.01)\n",
        "    ocsvm.fit(X_pca)\n",
        "    oc_preds = ocsvm.predict(X_pca)\n",
        "    df[\"ocsvm_anomaly\"] = oc_preds\n",
        "    pd.DataFrame({\"ocsvm_pred\": oc_preds}).to_csv(os.path.join(OUTPUT_DIR, \"ocsvm_preds.csv\"), index=False)\n",
        "except Exception as e:\n",
        "    print(\"OneClassSVM failed:\", e)\n",
        "\n",
        "# ---------- Association rules (optional) ----------\n",
        "if apriori is not None and association_rules is not None:\n",
        "    # pick a few categorical cols for association mining if available\n",
        "    ar_cols = [c for c in [\"Medication\", \"Medical Condition\", \"Hospital\", \"Insurance Provider\", \"Blood Type\"] if c in df.columns and c in categorical_cols] # check against updated categorical_cols\n",
        "    if ar_cols:\n",
        "        trans = pd.DataFrame(index=df.index)\n",
        "        for c in ar_cols:\n",
        "            dummies = pd.get_dummies(df[c].fillna(\"NA\").astype(str), prefix=c)\n",
        "            trans = pd.concat([trans, dummies], axis=1)\n",
        "        # limit to not too wide\n",
        "        if trans.shape[1] > 500:\n",
        "            trans = trans.iloc[:, :500]\n",
        "        freq_items = apriori(trans, min_support=0.01, use_colnames=True)\n",
        "        rules = association_rules(freq_items, metric=\"lift\", min_threshold=1.2)\n",
        "        freq_items.to_csv(os.path.join(OUTPUT_DIR, \"apriori_itemsets.csv\"), index=False)\n",
        "        rules.to_csv(os.path.join(OUTPUT_DIR, \"apriori_rules.csv\"), index=False)\n",
        "        print(\"Saved apriori outputs.\")\n",
        "else:\n",
        "    print(\"mlxtend not available - skipping association rules (you can install mlxtend).\")\n",
        "\n",
        "# ---------- Save summaries & outputs ----------\n",
        "pd.DataFrame(cluster_summary).to_csv(os.path.join(OUTPUT_DIR, \"cluster_summary_overview.csv\"), index=False)\n",
        "print(\"Saved cluster_summary_overview.csv\")\n",
        "\n",
        "# Choose a clustering column to explain (prefer kmeans_k4 if available)\n",
        "cluster_cols = [c for c in df.columns if c.endswith(\"_cluster\")]\n",
        "chosen_col = None\n",
        "for pref in [\"kmeans_k4_cluster\", \"kmeans_k5_cluster\", \"kmeans_k3_cluster\"]:\n",
        "    if pref in df.columns:\n",
        "        chosen_col = pref\n",
        "        break\n",
        "if chosen_col is None and cluster_cols:\n",
        "    chosen_col = cluster_cols[0]\n",
        "\n",
        "if chosen_col:\n",
        "    print(\"Explaining clusters using:\", chosen_col)\n",
        "    cluster_feature_summary = []\n",
        "    for cl in sorted(df[chosen_col].unique()):\n",
        "        sub = df[df[chosen_col] == cl]\n",
        "        row = {\"cluster\": int(cl) if pd.notna(cl) else \"nan\", \"size\": len(sub)}\n",
        "        # numeric means\n",
        "        for n in numeric_cols:\n",
        "            if n in sub.columns:\n",
        "                row[f\"mean__{n}\"] = sub[n].mean()\n",
        "        # top categorical\n",
        "        for c in categorical_cols:\n",
        "            if c in sub.columns:\n",
        "                row[f\"top__{c}\"] = sub[c].value_counts().idxmax() if not sub[c].value_counts().empty else None\n",
        "        cluster_feature_summary.append(row)\n",
        "    pd.DataFrame(cluster_feature_summary).to_csv(os.path.join(OUTPUT_DIR, \"cluster_feature_summary.csv\"), index=False)\n",
        "    print(\"Saved cluster_feature_summary.csv\")\n",
        "\n",
        "# Save dataframe with clusters & embeddings\n",
        "out_df = df.copy()\n",
        "out_df[\"pca_dim1\"] = X_2d[:,0]\n",
        "out_df[\"pca_dim2\"] = X_2d[:,1]\n",
        "out_df.to_csv(os.path.join(OUTPUT_DIR, \"df_with_clusters_and_embeddings.csv\"), index=False)\n",
        "print(\"Saved df_with_clusters_and_embeddings.csv and many PNGs in\", OUTPUT_DIR)\n",
        "\n",
        "print(\"DONE. Check the folder\", OUTPUT_DIR, \"for outputs (CSVs + PNGs).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zibjKP4MxGH",
        "outputId": "bb8cdb97-44d7-49c3-b951-5fced92b4b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file: Healthcare dataset.zip\n",
            "Loading CSV: /tmp/healthcare_extract/healthcare_dataset.csv\n",
            "Rows loaded: 55500\n",
            "Sampled rows: 4000\n",
            "Numeric cols: ['Age', 'Billing Amount', 'LOS_Days', 'Admission_Year', 'Admission_Month', 'Admission_DOW']\n",
            "Categorical cols: ['Gender', 'Blood Type', 'Medical Condition', 'Hospital', 'Insurance Provider', 'Admission Type', 'Medication', 'Test Results']\n",
            "Reduced cardinality for Hospital: now 51 uniques\n",
            "Feature matrix shape (after encoding): (4000, 89)\n",
            "PCA explained variance (cumulative first 5): [0.11510559 0.20632348 0.29667743 0.38537328 0.47255495]\n",
            "Saved apriori outputs.\n",
            "Saved cluster_summary_overview.csv\n",
            "Explaining clusters using: kmeans_k4_cluster\n",
            "Saved cluster_feature_summary.csv\n",
            "Saved df_with_clusters_and_embeddings.csv and many PNGs in unsupervised_reports\n",
            "DONE. Check the folder unsupervised_reports for outputs (CSVs + PNGs).\n"
          ]
        }
      ]
    }
  ]
}